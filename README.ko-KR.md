<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/NLPOptimize/flash-tokenizer/blob/main/assets/FlashTokenizer_main_dark.png?raw=true">
    <img alt="FlashTokenizer" src="https://github.com/NLPOptimize/flash-tokenizer/blob/main/assets/FlashTokenizer_main_light.png?raw=true" width=60%>
  </picture>
</p>
<h1 align="center">
The world's fastest CPU tokenizer library!
</h1>


## EFFICIENT AND OPTIMIZED TOKENIZER ENGINE FOR LLM INFERENCE SERVING

[FlashTokenizer](https://pypi.org/project/flash-tokenizer/)ëŠ” **LLM ì¶”ë¡ ì— ì‚¬ìš©ë˜ëŠ” BertTokenizerë¥¼ C++ë¡œ ê³ ì„±ëŠ¥ êµ¬í˜„í•œ í† í¬ë‚˜ì´ì €**ì…ë‹ˆë‹¤. [FlashAttention](https://github.com/Dao-AILab/flash-attention)ê³¼ [FlashInfer](https://github.com/flashinfer-ai/flashinfer)ì™€ ê°™ì´ ìµœê³ ì˜ ì†ë„ì™€ ì •í™•ì„±ì„ ì œê³µí•˜ë©°, transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `BertTokenizerFast`ë³´ë‹¤ **10ë°° ë” ë¹ ë¦…ë‹ˆë‹¤**.



### [â–¶ï¸ ì„±ëŠ¥ ë¹„êµ ë°ëª¨](https://www.youtube.com/watch?v=a_sTiAXeSE0)



> [!NOTE]  
> ### FlashTokenizerë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ .
> - ë§ì€ ê°œë°œìë“¤ì€ [Huggingfaceì˜ BertTokenizerFast](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert_fast.py)ë³´ë‹¤ ë” ë¹ ë¥´ê³ , ë” ì •í™•í•˜ë©°, ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í† í¬ë‚˜ì´ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤. ([ë§í¬1](https://stackoverflow.com/questions/75595699/huggingfaces-berttokenizerfast-is-between-39000-and-258300-times-slower-than-ex), [ë§í¬2](https://github.com/PaddlePaddle/PaddleNLP/issues/8565), [ë§í¬3](https://blog.csdn.net/xhw205/article/details/129578988))
> - [PaddleNLPì˜ BertTokenizerFast](https://paddlenlp.readthedocs.io/en/stable/_modules/paddlenlp/experimental/faster_tokenizer.html)ëŠ” [Huggingfaceì˜ Rust ë²„ì „](https://github.com/huggingface/tokenizers)ì„ `C++`ë¡œ êµ¬í˜„í•˜ì—¬ ì•½ 1.2ë°°ì˜ ì†ë„ í–¥ìƒì„ ì´ë£¨ì—ˆì§€ë§Œ, ì´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë§¤ìš° ë¬´ê±°ìš´ [PaddlePaddle](https://github.com/PaddlePaddle/Paddle)ê³¼ [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
> - [Tensorflow-textì˜ FastBertTokenizer](https://www.tensorflow.org/text/api_docs/python/text/FastBertTokenizer)ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë” ëŠë¦½ë‹ˆë‹¤.
> - [Microsoftì˜ Blingfire](https://github.com/microsoft/BlingFire)ëŠ” custom ë°ì´í„°ë¥¼ ì´ìš©í•œ ë¹Œë“œì— **8ì‹œê°„ ì´ìƒì´ ê±¸ë¦¬ë©°**, ìƒëŒ€ì ìœ¼ë¡œ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.
> - [Rapidì˜ cuDF](https://github.com/rapidsai/cudf)ëŠ” GPU ê¸°ë°˜ì˜ BertTokenizerë¥¼ ì œê³µí•˜ì§€ë§Œ ì •í™•ë„ì˜ ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
> - ë˜í•œ, ì•ˆíƒ€ê¹ê²Œë„, `C#`ìœ¼ë¡œ ê°œë°œëœ [FastBertTokenizer](https://github.com/georg-jung/FastBertTokenizer)ì™€ [BertTokenizers](https://github.com/NMZivkovic/BertTokenizers)ëŠ” `Python` í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
> 
> - ê·¸ë˜ì„œ  **FlashTokenizer**ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” ê°„ë‹¨íˆ `pip`ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆê³ , **ìœ ì§€ë³´ìˆ˜ê°€ ì‰¬ìš´ C++ë¡œ ê°œë°œ**ë˜ì—ˆìœ¼ë©° ë§¤ìš° ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. ì´ëŠ” Blingfireë³´ë‹¤ ë¹ ë¥´ê³  ì‚¬ìš©í•˜ê¸° í¸ë¦¬í•˜ê²Œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.. FlashTokenizerëŠ” [Fast WordPiece Tokenization](https://arxiv.org/abs/2012.15524) ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ **LinMax Tokenizer**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜• ì‹œê°„ì— í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë˜í•œ C++ ìˆ˜ì¤€ì—ì„œ **ë³‘ë ¬ ì²˜ë¦¬ë¡œ batch encodingì„ ì§€ì›**í•˜ì—¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.




<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/NLPOptimize/flash-tokenizer/blob/main/assets/Banner_dark.png?raw=true">
    <img alt="Banner" src="https://github.com/NLPOptimize/flash-tokenizer/blob/main/assets/Banner_light.png?raw=true" width=100%>
  </picture>
</p>


<p>
<img align="left" src="https://img.shields.io/badge/success-0B86F1?style=flat&logo=python&logoColor=white&label=MacOS_build">
<img align="left" src="https://img.shields.io/badge/success-0B86F1?style=flat&logo=python&logoColor=white&label=Windows_build">
<img align="left" src="https://img.shields.io/badge/success-0B86F1?style=flat&logo=python&logoColor=white&label=Linux_build">
</p><br>

* * *

### FlashTokenizerì˜ í•µì‹¬ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

> [!TIP]
> 
> * C++17ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
>   * **MacOS**: `clang++`
>   * **Windows**: `Visual Studio 2022`
>   * **Ubuntu**: `g++`
> 
> * pybind11ì„ í†µí•´ Pythonì—ì„œë„ ë™ì¼í•˜ê²Œ ë¹ ë¥¸ ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
> * OPENMPë¥¼ í™œìš©í•˜ì—¬ C++ ìˆ˜ì¤€ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.



## ë‰´ìŠ¤

> [!IMPORTANT]  
> **[2025ë…„ 4ì›” 2ì¼]**
> - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
> - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ëŠ” Pythonìœ¼ë¡œ ìˆ˜í–‰ë˜ë©°, í•„ìš”í•œ íŒ¨í‚¤ì§€ëŠ” [setup.sh](./perftest/setup.sh)ë¥¼ í†µí•´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - `BasicTokenizer`ì— `tokenize_early_stop` ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ì†Œí­ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
> - Windows, Linux, macOS í™˜ê²½ ëª¨ë‘ì—ì„œ [OpenMP](https://www.openmp.org/)ê°€ `std::thread`ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ OpenMPë¡œ ì „ë¶€ ì „í™˜í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 31ì¼]**
> - ê° ìš´ì˜ì²´ì œ(OS)ë³„ë¡œ ë¯¸ë¦¬ ë¹Œë“œëœ whl íŒŒì¼ì„ ì œê³µí•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 22ì¼]**
> - AC Trieì— [DFA](https://blog.cloudflare.com/pt-br/making-waf-ai-models-go-brr/#:~:text=We%20can%20also%20tune%20Aho,settings%20based%20on%20this%20recommendation)ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 21ì¼]**
> - í† í¬ë‚˜ì´ì € ì •í™•ë„ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 19ì¼]** 
> - [Ahoâ€“Corasick ì•Œê³ ë¦¬ì¦˜](https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm)ì„ ì´ìš©í•œ LinMaxMatching ê¸°ë²• ì ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
> - ëª¨ë“  í•¨ìˆ˜ì˜ ë¸Œëœì¹˜ íŒŒì´í”„ë¼ì´ë‹(branch pipelining)ì„ ê°œì„ í•˜ê³ , ê°•ì œë¡œ ì¸ë¼ì¸(force-inline)ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
> - `WordpieceTokenizer(Backward)`ì˜ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
> - [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter)ë¥¼ ì œì™¸í•˜ê³ ëŠ” ìºì‹±ë³´ë‹¤ ë¹ ë¥´ë„ë¡ ëª¨ë“  í•¨ìˆ˜ë¥¼ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.
> - `punctuation`, `control`, `whitespace`ë¥¼ ë¯¸ë¦¬ constexprë¡œ ì •ì˜í•˜ê³  ë¸”ë£¸ í•„í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
> - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ì„ í†µí•´ ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ í• ë‹¹ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤.
> - âœ¨FlashTokenizerâœ¨ëŠ” `bert-base-uncased` ëª¨ë¸ ê¸°ì¤€ ë‹¨ì¼ ì½”ì–´ì—ì„œ ì´ˆë‹¹ ì•½ **35,000ê°œì˜ í…ìŠ¤íŠ¸**ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ í•˜ë‚˜ë‹¹ ì•½ **28ns**ì˜ ì²˜ë¦¬ ì†ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 18ì¼]**
> - BasicTokenizerì˜ ì •í™•ë„ê°€ í–¥ìƒë˜ì–´ ì „ì²´ì ì¸ í† í¬ë‚˜ì´ì € ì •í™•ë„ê°€ ê°œì„ ë˜ì—ˆìœ¼ë©°, íŠ¹íˆ ìœ ë‹ˆì½”ë“œ ì…ë ¥ì— ëŒ€í•´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 14ì¼]**
> - [Fast WordPiece Tokenization](https://arxiv.org/abs/2012.15524) ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ [Trie](https://en.wikipedia.org/wiki/Trie)ë¥¼ ì‚¬ìš©í•˜ì—¬ `WordPieceTokenizer`ì™€ `WordPieceBackwardTokenizer`ì˜ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
> - SingleEncodingì—ì„œëŠ” `std::list`ì— `FastPoolAllocator`ë¥¼ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì˜€ìœ¼ë‚˜, ì´ëŠ” thread-safe í•˜ì§€ ì•Šì•„ BatchEncodingì—ì„œëŠ” ì¼ë°˜ `std::list<std::string>`ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. BatchEncodingì—ì„œëŠ” `OPENMP`ë¥¼ ì™„ì „íˆ ì œê±°í•˜ê³  ì˜¤ì§ `std::thread`ë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 10ì¼]**
> - robin_hoodë¥¼ í™œìš©í•œ ë¹ ë¥¸ í† í° ë§¤í•‘ê³¼ ë©”ëª¨ë¦¬ ë³µì‚¬ë¥¼ ìµœì†Œí™”í•œ `std::list`ë¥¼ í†µí•´ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
>
> #### í† í° ID ë§µ í…Œì´ë¸” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
> - í† í° ë° ID ë§¤í•‘ì—ëŠ” ê°€ì¥ ë¹ ë¥¸ ì„±ëŠ¥ì„ ë³´ì¸ `robin_hood::unordered_flat_map<std::string, int>`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
>
> **[2025ë…„ 3ì›” 9ì¼]** BertTokenizerë¥¼ ìœ„í•œ flash-tokenizer ê°œë°œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
>

## 1. ì„¤ì¹˜

### ìš”êµ¬ ì‚¬í•­
 * `Windows(AMD64)`, `MacOS(ARM64)`, `Ubuntu(x86-64)` .
 * `g++` / `clang++` / `MSVC`.
 * `python 3.8 ~ 3.13`.

### [PIP](https://pypi.org/project/flash-tokenizer/) ìœ¼ë¡œ ì„¤ì¹˜í•˜ê¸°.


Windowsì—ì„œëŠ”  [vc_redist.x64.exe](https://github.com/NLPOptimize/flash-tokenizer/releases/download/Packages/VC_redist.x64.exe) ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```bash
# Windows
pip install -U flash-tokenizer
```
```bash
# Linux
pip install -U flash-tokenizer
```
```bash
# MacOS
pip install -U flash-tokenizer
```

### Sourceì—ì„œ ì§ì ‘ ë¹Œë“œí•˜ê¸°.
```bash
git clone https://github.com/NLPOptimize/flash-tokenizer
cd flash-tokenizer/prj
pip install .
```


## 2. ì˜ˆì œ

```python
from flash_tokenizer import BertTokenizerFlash
from transformers import BertTokenizer

titles = [
    'ç»ä¸èƒ½æ”¾å¼ƒï¼Œä¸–ç•Œä¸Šæ²¡æœ‰å¤±è´¥ï¼Œåªæœ‰æ”¾å¼ƒã€‚',
    'is there any doubt about it "None whatsoever"',
    "ì„¸ìƒ ì–´ë–¤ ì§ìŠ¹ì´ ì´ë¥¼ ë“œëŸ¬ë‚´ê³  ì‚¬ëƒ¥ì„ í•´? ì•½í•œ ì§ìŠ¹ì´ë‚˜ ëª¸ì„ ë¶€í’€ë¦¬ì§€, ì§„ì§œ ì§ìŠ¹ì€ ëˆ„êµ¬ë³´ë‹¤ ì¹¨ì°©í•˜ì§€.",
    'ãã®ã‚ˆã†ã«äºŒç•ªç›®ã«æ­»ã‚’å½è£…ã—ã¦ç”Ÿãæ®‹ã‚‹ã‚ˆã†ã«ãªã£ãŸã‚¤ã‚¿ãƒ‰ãƒªãŒã©ã†ã—ã¦åˆã‚ã¦è¦‹ã‚‹è‡ªåˆ†ã‚’ã“ã‚“ãªã«æ°—é£ã£ã¦ãã‚Œã‚‹ã®ã‹ã¨å°‹ã­ã‚‹ã¨ã€Œç§ãŒå¤§åˆ‡ã«ã™ã‚‹äººãŸã¡ãŒã‚ãªãŸã‚’å¤§åˆ‡ã«ã™ã‚‹ã‹ã‚‰ã€ã¨ç­”ãˆã¦ã¯'
]

tokenizer1 = BertTokenizerFlash.from_pretrained('bert-base-multilingual-cased')
tokenizer2 = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

correct = 0
for title in titles:
    print(title)
    tokens1 = tokenizer1.tokenize(title)
    tokens2 = tokenizer2.tokenize(title)
    ids1 = tokenizer1(title, max_length=512, padding="longest").input_ids[0]
    ids2 = tokenizer2(title, max_length=512, padding="longest", return_tensors="np").input_ids[0].tolist()
    if tokens1 == tokens2 and ids1 == ids2:
        correct += 1
        print("Accept!")
    else:
        print("Wrong Answer")
    print(ids1)
    print(ids2)
    print()

print(f'Accuracy: {correct * 100.0 / len(titles):.2f}%')
```


```
ç»ä¸èƒ½æ”¾å¼ƒï¼Œä¸–ç•Œä¸Šæ²¡æœ‰å¤±è´¥ï¼Œåªæœ‰æ”¾å¼ƒã€‚
Accept!
[101, 6346, 2080, 6546, 4284, 3704, 10064, 2087, 5621, 2078, 4917, 4461, 3204, 7480, 10064, 2751, 4461, 4284, 3704, 1882, 102]
[101, 6346, 2080, 6546, 4284, 3704, 10064, 2087, 5621, 2078, 4917, 4461, 3204, 7480, 10064, 2751, 4461, 4284, 3704, 1882, 102]

is there any doubt about it "None whatsoever"
Accept!
[101, 10124, 11155, 11178, 86697, 10978, 10271, 107, 86481, 12976, 11669, 23433, 107, 102]
[101, 10124, 11155, 11178, 86697, 10978, 10271, 107, 86481, 12976, 11669, 23433, 107, 102]

ì„¸ìƒ ì–´ë–¤ ì§ìŠ¹ì´ ì´ë¥¼ ë“œëŸ¬ë‚´ê³  ì‚¬ëƒ¥ì„ í•´? ì•½í•œ ì§ìŠ¹ì´ë‚˜ ëª¸ì„ ë¶€í’€ë¦¬ì§€, ì§„ì§œ ì§ìŠ¹ì€ ëˆ„êµ¬ë³´ë‹¤ ì¹¨ì°©í•˜ì§€.
Accept!
[101, 9435, 14871, 55910, 9710, 48210, 10739, 35756, 9113, 30873, 31605, 11664, 9405, 118729, 10622, 9960, 136, 9539, 11102, 9710, 48210, 43739, 9288, 10622, 9365, 119407, 12692, 12508, 117, 9708, 119235, 9710, 48210, 10892, 9032, 17196, 80001, 9783, 119248, 23665, 119, 102]
[101, 9435, 14871, 55910, 9710, 48210, 10739, 35756, 9113, 30873, 31605, 11664, 9405, 118729, 10622, 9960, 136, 9539, 11102, 9710, 48210, 43739, 9288, 10622, 9365, 119407, 12692, 12508, 117, 9708, 119235, 9710, 48210, 10892, 9032, 17196, 80001, 9783, 119248, 23665, 119, 102]

ãã®ã‚ˆã†ã«äºŒç•ªç›®ã«æ­»ã‚’å½è£…ã—ã¦ç”Ÿãæ®‹ã‚‹ã‚ˆã†ã«ãªã£ãŸã‚¤ã‚¿ãƒ‰ãƒªãŒã©ã†ã—ã¦åˆã‚ã¦è¦‹ã‚‹è‡ªåˆ†ã‚’ã“ã‚“ãªã«æ°—é£ã£ã¦ãã‚Œã‚‹ã®ã‹ã¨å°‹ã­ã‚‹ã¨ã€Œç§ãŒå¤§åˆ‡ã«ã™ã‚‹äººãŸã¡ãŒã‚ãªãŸã‚’å¤§åˆ‡ã«ã™ã‚‹ã‹ã‚‰ã€ã¨ç­”ãˆã¦ã¯
Accept!
[101, 11332, 24273, 2150, 5632, 5755, 1943, 4805, 1980, 2371, 7104, 11592, 5600, 1913, 4814, 1975, 27969, 15970, 21462, 15713, 21612, 10898, 56910, 22526, 22267, 2547, 19945, 7143, 1975, 6621, 2534, 1980, 28442, 60907, 11312, 4854, 7770, 14813, 18825, 58174, 75191, 11662, 3456, 1945, 100812, 1890, 5949, 1912, 3197, 2535, 84543, 2179, 78776, 111787, 22946, 20058, 11377, 3197, 2535, 84543, 16867, 1891, 1940, 6076, 27144, 11588, 102]
[101, 11332, 24273, 2150, 5632, 5755, 1943, 4805, 1980, 2371, 7104, 11592, 5600, 1913, 4814, 1975, 27969, 15970, 21462, 15713, 21612, 10898, 56910, 22526, 22267, 2547, 19945, 7143, 1975, 6621, 2534, 1980, 28442, 60907, 11312, 4854, 7770, 14813, 18825, 58174, 75191, 11662, 3456, 1945, 100812, 1890, 5949, 1912, 3197, 2535, 84543, 2179, 78776, 111787, 22946, 20058, 11377, 3197, 2535, 84543, 16867, 1891, 1940, 6076, 27144, 11588, 102]

Accuracy: 100.00%
```

## 3. ë‹¤ë¥¸ êµ¬í˜„ì²´ë“¤.


<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="./assets/logos_dark.png">
    <img alt="Banner" src="./assets/logos_light.png" width=100%>
  </picture>
</p>


ëŒ€ë¶€ë¶„ì˜ [BERT](https://arxiv.org/abs/1810.04805) ê¸°ë°˜ ëª¨ë¸ë“¤ì€ [WordPiece í† í¬ë‚˜ì´ì €](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ê·¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/google-research/bert/blob/master/tokenization.py)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Huggingfaceì˜ ê°„ë‹¨í•œ êµ¬í˜„ì²´ëŠ” [ì—¬ê¸°](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py)ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

BertTokenizerëŠ” CPU ì—°ì‚°ëŸ‰ì´ ë§ê¸° ë•Œë¬¸ì— ì¶”ë¡  ë‹¨ê³„ì—ì„œ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìœ¼ë©°, ìµœì í™”ë˜ì§€ ì•Šì€ í† í¬ë‚˜ì´ì €ëŠ” ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì‚¬ë¡€ë¡œ [KR-BERT](https://arxiv.org/abs/2008.03979)ì—ì„œ ì†Œê°œëœ [BidirectionalWordpieceTokenizer](https://github.com/snunlp/KR-BERT/blob/master/krbert_tensorflow/tokenization_ranked.py)ê°€ ìˆìŠµë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ì˜ ëŒ€ë¶€ë¶„ ì½”ë“œëŠ” ê¸°ì¡´ê³¼ ê°™ì§€ë§Œ, í•˜ìœ„ í† í°(sub token)ì„ ì—­ë°©í–¥ìœ¼ë¡œ íƒìƒ‰í•˜ë©°, ì •ë°©í–¥ íƒìƒ‰ì— ë¹„í•´ ë” í° ê°’ì„ ê¸°ë¡í•©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ í†µí•´ ì •í™•ë„ê°€ ê°œì„ ëœë‹¤ê³  ì£¼ì¥í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì •ëŸ‰ì  ì§€í‘œë¥¼ ì°¾ê¸°ëŠ” ì–´ë µê³ , ì‹¤ì œ ì •í™•ë„ì˜ ê°œì„ ë„ í¬ì§€ ì•Šì€ ë°˜ë©´ í† í¬ë‚˜ì´ì €ì˜ ì†ë„ê°€ ì‹¬ê°í•˜ê²Œ ëŠë ¤ì§‘ë‹ˆë‹¤.

ë‹¤ìŒì€ ì£¼ìš” í† í¬ë‚˜ì´ì €ì˜ êµ¬í˜„ ë°©ì‹ì…ë‹ˆë‹¤:

- transformers (Rust êµ¬í˜„ì²´, PyO3 ì‚¬ìš©)
- paddlenlp (C++ êµ¬í˜„ì²´, pybind ì‚¬ìš©)
- tensorflow-text (C++ êµ¬í˜„ì²´, pybind ì‚¬ìš©)
- blingfire (C++ êµ¬í˜„ì²´, ë„¤ì´í‹°ë¸Œ ë°”ì´ë„ˆë¦¬ í˜¸ì¶œ)

ëŒ€ë¶€ë¶„ì˜ ê°œë°œìëŠ” ë³´í†µ `transformers.BertTokenizer` ë˜ëŠ” `transformers.AutoTokenizer`ë¥¼ ì‚¬ìš©í•  í…ë°, ì´ë•Œ `AutoTokenizer`ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹¤ì œë¡œëŠ” `transformers.BertTokenizerFast`ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.

ì´ëŠ” ë‹¹ì—°íˆ ê¸°ì¡´ BertTokenizerë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ, ê²°ê³¼ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì¦‰, í† í¬ë‚˜ì´ì €ë¶€í„° ì´ë¯¸ 100%ì˜ ì •í™•ë„ë¥¼ í¬ê¸°í•˜ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

BertTokenizerëŠ” transformers ì™¸ì—ë„ [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)ì™€ [tensorflow-text](https://www.tensorflow.org/text)ì—ì„œë„ ì œê³µë©ë‹ˆë‹¤.

í•œí¸ [Blingfire](https://github.com/microsoft/BlingFire)ëŠ” ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ê°œë°œí–ˆìœ¼ë‚˜ í˜„ì¬ëŠ” ìœ ì§€ë³´ìˆ˜ê°€ ì¤‘ë‹¨ëœ ìƒíƒœì…ë‹ˆë‹¤.

PaddleNLPëŠ” PaddlePaddleì´ í•„ìš”í•˜ë©°, 3.0rc ë²„ì „ë¶€í„° í† í¬ë‚˜ì´ì € ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì„¤ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
##### Install PaddlePaddle, PaddleNLP
python -m pip install paddlepaddle==3.0.0b1 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/
pip install --upgrade paddlenlp==3.0.0b3
##### Install transformers
pip install transformers==4.47.1
##### Install tf-text
pip install tensorflow-text==2.18.1
##### Install blingfire
pip install blingfire
```

blingfireë¥¼ ì œì™¸í•˜ë©´, ëŒ€ë¶€ë¶„ì˜ BertTokenizer êµ¬í˜„ì²´ëŠ” `vocab.txt`ë§Œ ìˆìœ¼ë©´ ì¦‰ì‹œ tokenizerë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
(blingfire ì—­ì‹œ `vocab.txt`ë§Œ ìˆìœ¼ë©´ ë˜ì§€ë§Œ, ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì‚¬ì „ í•™ìŠµ ê³¼ì •ì´ ì•½ 8ì‹œê°„ í•„ìš”í•©ë‹ˆë‹¤.)

ìì„¸íˆ ì‚´í´ë³¼ êµ¬í˜„ì²´ëŠ” `PaddleNLPì˜ BertTokenizerFast`ì™€ `blingfire`ì…ë‹ˆë‹¤.

- **blingfire**: [ê²°ì •ì  ìœ í•œ ìƒíƒœ ë¨¸ì‹ (DFSM)](https://github.com/microsoft/BlingFire/blob/master/doc/Bling_Fire_Tokenizer_Algorithms.pdf)ì„ í™œìš©í•˜ì—¬ ì„ í˜• íƒìƒ‰ì„ í•œ ë²ˆìœ¼ë¡œ ì¤„ì´ê³  ë¶ˆí•„ìš”í•œ ë¹„êµ ì—°ì‚°ì„ ì œê±°í•´, O(n)ì˜ ìš°ìˆ˜í•œ ì‹œê°„ ë³µì¡ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
  - **ì¥ì **: ë‹¤ë¥¸ êµ¬í˜„ì²´ë³´ë‹¤ **5~10ë°° ë¹ ë¥¸ ì†ë„**.
  - **ë‹¨ì **: ê¸´ í•™ìŠµ ì‹œê°„(ì•½ 8ì‹œê°„)ì´ í•„ìš”í•˜ê³ , ì •í™•ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŠµë‹ˆë‹¤.  
    (ì‚¬ì‹¤ìƒ ê°œë°œì´ ì¤‘ë‹¨ë˜ì–´ ë„ì›€ì„ ì–»ê¸° ì–´ë µìŠµë‹ˆë‹¤.)

- **PaddleNLP**: ì•„ë˜ì˜ ì‹¤í—˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, PaddleNLPëŠ” Huggingfaceì˜ BertTokenizerFastë³´ë‹¤ ì–¸ì œë‚˜ ë” ë¹ ë¥¸ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì†Œìˆ˜ì  ë‹¨ìœ„ë¡œ ë¹„êµí•´ë„ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” X86ì´ë‚˜ Arm ë“± OSì™€ ìƒê´€ì—†ì´ ë™ì¼í•©ë‹ˆë‹¤.
  - **ì¥ì **: ë‚´ë¶€ì ìœ¼ë¡œ **C++ë¡œ êµ¬í˜„**ë˜ì–´ ìˆì–´ Rustë¡œ êµ¬í˜„ëœ Huggingfaceì˜ `transformers.BertTokenizerFast`ë³´ë‹¤ 1.2ë°° ë¹ ë¥´ë©´ì„œë„ ê²°ê³¼ ê°’ì€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•©ë‹ˆë‹¤.
    - ë‹¤ë§Œ `return_tensors` ì˜µì…˜ì—ì„œ PyTorch í…ì„œ(`pt`)ë¥¼ ì§ì ‘ ì§€ì •í•  ìˆ˜ ì—†ì§€ë§Œ, í° ë¬¸ì œëŠ” ì•„ë‹™ë‹ˆë‹ˆë‹¤.
  - **ë‹¨ì **: PaddlePaddleê³¼ PaddleNLP íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•œë‹¤ëŠ” ì  ì™¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤.

## 4. ì„±ëŠ¥ í‰ê°€


### 4.1 ì„±ëŠ¥ ì¸¡ì • (Single text encoding)

Acc
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="./assets/comp_speed_dark.png">
    <img alt="FlashTokenizer" src="./assets/comp_speed_light.png" width=100%>
  </picture>
</p>

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="./assets/comp_accuracy_dark.png">
    <img alt="FlashTokenizer" src="./assets/comp_accuracy_light.png" width=100%>
  </picture>
</p>


### Tokenizer ì„±ëŠ¥ ë¹„êµ í‘œ

Accuracy ëŠ” [Googleì˜ BertTokenizerFast](https://github.com/google-research/bert/blob/master/tokenization.py)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¸¡ì •í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë§Œì•½ ë‹¨ í•˜ë‚˜ì˜ `input_ids` ê°’ì´ë¼ë„ ê¸°ì¤€ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´, í•´ë‹¹ ê²°ê³¼ëŠ” ì˜¤ë‹µìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.

#### [google-bert/bert-base-cased](https://huggingface.co/google-bert/bert-base-cased)

| Tokenizer                      | Elapsed Time | texts     | Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerFast(Huggingface) | 84.3700s     | 1,000,000 | 99.9226% |
| BertTokenizerFast(PaddleNLP)   | 75.6551s     | 1,000,000 | 99.9226% |
| FastBertTokenizer(Tensorflow)  | 219.1259s    | 1,000,000 | 99.9160% |
| Blingfire                      | 13.6183s     | 1,000,000 | 99.8991% |
| **FlashBertTokenizer**             | 8.1968s      | 1,000,000 | 99.8216% |

#### [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)

| Tokenizer                      |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerFast(Huggingface) |       91.7882s | 1,000,000 |   99.9326% |
| BertTokenizerFast(PaddleNLP)   |       83.6839s | 1,000,000 |   99.9326% |
| FastBertTokenizer(Tensorflow)  |      204.2240s | 1,000,000 |   99.1379% |
| Blingfire                      |       13.2374s | 1,000,000 |   99.8588% |
| **FlashBertTokenizer**             |        7.6313s | 1,000,000 |   99.6884% |

#### [google-bert/bert-base-multilingual-cased](https://huggingface.co/google-bert/bert-base-multilingual-cased)



| Tokenizer                      | Elapsed Time | texts     | Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerFast(Huggingface) | 212.1570s    | 2,000,000 | 99.7964% |
| BertTokenizerFast(PaddleNLP)   | 193.9921s    | 2,000,000 | 99.7964% |
| FastBertTokenizer(Tensorflow)  | 394.1574s    | 2,000,000 | 99.7892% |
| Blingfire                      | 38.9013s     | 2,000,000 | 99.9780% |
| **FlashBertTokenizer**             | 20.4570s     | 2,000,000 | 99.8970% |


#### [beomi/kcbert-base](https://github.com/Beomi/KcBERT)

| Tokenizer                      |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerFast(Huggingface) |       52.5744s | 1,000,000 |   99.6754% |
| BertTokenizerFast(PaddleNLP)   |       44.8943s | 1,000,000 |   99.6754% |
| FastBertTokenizer(Tensorflow)  |      198.0270s | 1,000,000 |   99.6639% |
| Blingfire                      |       13.0701s | 1,000,000 |   99.9434% |
| **FlashBertTokenizer**             |        5.2601s | 1,000,000 |   99.9484% |


| Tokenizer                      |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------|-----------|------------|
| **FlashBertTokenizer**             |        5.1875s | 1,000,001 |   99.9484% |
| Blingfire                      |       13.2783s | 1,000,001 |   99.9435% |
| rust_tokenizers(guillaume-be)  |       16.6308s | 1,000,001 |   99.9829% |
| BertTokenizerFast(PaddleNLP)   |       44.5476s | 1,000,001 |   99.6754% |
| BertTokenizerFast(Huggingface) |       53.2525s | 1,000,001 |   99.6754% |
| FastBertTokenizer(Tensorflow)  |      202.1633s | 1,000,001 |   99.6639% |

#### [microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank](https://huggingface.co/microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank)

| Tokenizer                      |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerFast(Huggingface) |      208.8858s | 2,000,000 |   99.7964% |
| BertTokenizerFast(PaddleNLP)   |      192.6593s | 2,000,000 |   99.7964% |
| FastBertTokenizer(Tensorflow)  |      413.2010s | 2,000,000 |   99.7892% |
| Blingfire                      |       39.3765s | 2,000,000 |   99.9780% |
| **FlashBertTokenizer**             |       22.8820s | 2,000,000 |   99.8970% |

| Tokenizer                      |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------|-----------|------------|
| **FlashBertTokenizer**             |       22.0901s | 2,000,001 |   99.8971% |
| Blingfire                      |       37.9836s | 2,000,001 |   99.9780% |
| rust_tokenizers(guillaume-be)  |       98.0366s | 2,000,001 |   99.9976% |
| BertTokenizerFast(PaddleNLP)   |      208.6889s | 2,000,001 |   99.7964% |
| BertTokenizerFast(Huggingface) |      219.2644s | 2,000,001 |   99.7964% |
| FastBertTokenizer(Tensorflow)  |      413.9725s | 2,000,001 |   99.7892% |

#### [KR-BERT](https://github.com/snunlp/KR-BERT)


| Tokenizer                                    |   Elapsed Time |     texts |   Accuracy |
|--------------------------------|----------------:|-----------:|------------:|
| BertTokenizerBidirectional(KR-BERT Original) |      128.3320s | 1,000,000 |  100.0000% |
| **FlashBertTokenizer(Bidirectional)**                           |       10.4492s | 1,000,000 |   99.9631% |



```mermaid
%%{ init: { "er" : { "layoutDirection" : "LR" } } }%%
erDiagram
    Text ||--o{ Preprocess : tokenize
    Preprocess o{--|| Inference : memcpy_h2d
    Inference o{--|| Postprocess : memcpy_d2h
```





## 6. í˜¸í™˜ì„±

FlashBertTokenizerëŠ” ì–´ë–¤ í”„ë ˆì„ì›Œí¬ì™€ë„ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLMì˜ ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ì„œëŠ” ê° í”„ë ˆì„ì›Œí¬ì˜ CUDA ë²„ì „ í˜¸í™˜ì„±ë„ ì¤‘ìš”í•©ë‹ˆë‹¤.

- [PyTorch](https://pytorch.org/)ëŠ” ë” ì´ìƒ condaë¥¼ í†µí•œ ì„¤ì¹˜ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- [ONNXRUNTIME](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x)ì€ CUDA ë²„ì „ë³„ë¡œ ë‚˜ë‰˜ì–´ ì œê³µë©ë‹ˆë‹¤.
- PyTorch ì—­ì‹œ CUDA 12.xë¥¼ ì§€ì›í•˜ì§€ ì•Šê³  ìµœì‹  ë²„ì „ì¸ CUDA 12.8ë¡œ ì´ë™í•˜ë ¤ê³  í•˜ê³  ìˆì§€ë§Œ, í˜„ì¬ í”„ë ˆì„ì›Œí¬ ì „ë°˜ì˜ ì¶”ì„¸ëŠ” CUDA 11.8ì„ ìœ ì§€í•˜ëŠ” ë°©í–¥ì…ë‹ˆë‹¤.
  - CUDA 12.xëŠ” ìµœì‹  GPUì¸ Hopperì™€ Blackwellì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, Voltaì™€ ê°™ì€ GPUì—ì„œëŠ” CUDA 11.8ì´ CUDA 12.xë³´ë‹¤ ë” ë¹ ë¦…ë‹ˆë‹¤.



| DL Framework | Version | OS   | CPU  | CUDA 11.8 | CUDA 12.3 | CUDA 12.4 | CUDA 12.6 | CUDA 12.8 |
| ------------ | ----|---- | ---- | --------- | ----|----- | --------- | --------- |
| PyTorch | 2.6| Linux, Windows | âšª|âšª|âŒ|âšª| âšª |    âŒ      |
| PyTorch | 2.7|Linux, Windows|âšª|âšª|âŒ|âŒ|âšª|âšª|
| ONNXRUNTIME(11) | 1.20.x| Linux, Windows|âšª|âšª|âŒ|âŒ|âŒ|âŒ|
| ONNXRUNTIME(12) | 1.20.x| Linux, Windows|âšª|âŒ|âšª|âšª|âšª|âšª|
| PaddlePaddle | 3.0-beta | Linux, Windows|âšª|âšª|âŒ|âŒ|âŒ|âŒ|


## 7. GPU ê¸°ë°˜ í† í¬ë‚˜ì´ì €

ë‹¤ìŒì€ [Run State of the Art NLP Workloads at Scale with RAPIDS, HuggingFace, and Dask](https://developer.nvidia.com/blog/run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask/#:~:text=,and%20then%20used%20in%20subsequent)ì—ì„œ ì„¤ëª…í•˜ëŠ” cuDFì˜ ì„¤ì¹˜ ë° ì‚¬ìš© ì˜ˆì œì…ë‹ˆë‹¤. *(ë§¤ìš° ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤)*

WordPiece Tokenizerë¥¼ GPUì—ì„œ [RAPIDS(cuDF)](https://docs.rapids.ai/)ë¥¼ ì´ìš©í•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- [êµ¬í˜„ì²´ ì½”ë“œ](https://github.com/rapidsai/cudf/blob/0e99ec3ec15b8b0ebe68bd884c7d22d600e9259e/python/cudf/cudf/core/wordpiece_tokenize.py#L10)
- [ì‚¬ìš© ì˜ˆì œ](https://github.com/rapidsai/cudf/blob/0e99ec3ec15b8b0ebe68bd884c7d22d600e9259e/python/cudf/cudf/tests/text/test_subword_tokenizer.py#L244)

[RAPIDS ì„¤ì¹˜ ë°©ë²•](https://docs.rapids.ai/install/)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, Linuxë§Œ ì§€ì›í•˜ë©°, CUDA ë²„ì „ë„ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë“¤ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìµœì ì˜ ì„ íƒì€ [docker](https://hub.docker.com/r/rapidsai/base)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë©°, GPUë¥¼ í†µí•œ ë°°ì¹˜(batch) ì²˜ë¦¬ì—ì„œëŠ” CPUë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ, ìŠ¤íŠ¸ë¦¬ë°(streaming) ì²˜ë¦¬ì—ì„œëŠ” ì˜¤íˆë ¤ CPUë³´ë‹¤ ëŠë¦½ë‹ˆë‹¤.

í•´ë‹¹ [ë¸”ë¡œê·¸](https://developer.nvidia.com/blog/run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask/#:~:text=,and then used in subsequent)ì—ëŠ” ì¢‹ì€ ì˜ˆì œ ì½”ë“œì™€ ì„¤ëª…ì´ ì œê³µë©ë‹ˆë‹¤. cuDFë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € `vocab.txt`ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ [hash_vocab](https://github.com/rapidsai/cudf/blob/branch-25.06/python/cudf/cudf/utils/hash_vocab_utils.py)ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì œëŠ” ì´ hash_vocab í•¨ìˆ˜ê°€ ë‹¤êµ­ì–´(multilingual)ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜ì–´ì™€ ì¤‘êµ­ì–´ ì™¸ì˜ ë‹¤ë¥¸ ì–¸ì–´ê°€ í¬í•¨ëœ vocabì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, cuDFì˜ WordpieceTokenizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

```python
import cudf
from cudf.utils.hash_vocab_utils import hash_vocab
hash_vocab('bert-base-cased-vocab.txt', 'voc_hash.txt')
```





## TODO

- [x] [BidirectionalWordPieceTokenizer](https://github.com/snunlp/KR-BERT/blob/master/krbert_tensorflow/tokenization_ranked.py)
- [x] BatchEncoder with Multithreading. 
- [x] Replace `std::list` to `boost::intrusive::list`.
- [x] ~~[MaxMatch-Dropout: Subword Regularization for WordPiece](https://arxiv.org/abs/2209.04126) Option.~~
- [x] Use stack memory for reduce memory allocation. (C-Style, [alloca](https://man7.org/linux/man-pages/man3/alloca.3.html), [_alloca](https://learn.microsoft.com/ko-kr/cpp/c-runtime-library/reference/alloca?view=msvc-170))
- [x] ~~Support for parallel processing option for single encode.~~
- [ ] `circle.ai`
  - [ ] Implement distribution of compiled wheel packages for installation.
- [ ] SIMD
- [ ] ~~CUDA Version.~~



## ê°ì‚¬ì˜ ë§

FlashTokenizerëŠ” [FlashAttention](https://github.com/Dao-AILab/flash-attention), [FlashInfer](https://github.com/flashinfer-ai/flashinfer), [FastBertTokenizer](https://github.com/georg-jung/FastBertTokenizer), [tokenizers-cpp](https://github.com/mlc-ai/tokenizers-cpp) í”„ë¡œì íŠ¸ì—ì„œ ì˜ê°ì„ ì–»ì–´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.



## ì„±ëŠ¥ ë¹„êµ

* **WordPiece**
  * ğŸ“’ [huggingface/tokenizers (Rust)](https://github.com/huggingface/tokenizers)
    * transformers.BertTokenizerFastì˜ Rust êµ¬í˜„ì²´ë¡œ, íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¡œ ì œê³µë©ë‹ˆë‹¤.
    * ğŸ”µ **pipìœ¼ë¡œ ì„¤ì¹˜ ê°€ëŠ¥**
  * ğŸ”¥ [FastBertTokenizer (C#)](https://fastberttokenizer.gjung.com)
    * ë§¤ìš° ë¹ ë¥¸ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì˜ì–´ê°€ ì•„ë‹Œ ì…ë ¥ì—ì„œëŠ” ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤.
  * âŒ [BertTokenizers (C#)](https://github.com/NMZivkovic/BertTokenizers)
    * [FastBertTokenizer (C#) VS BertTokenizers (C#)](https://github.com/georg-jung/FastBertTokenizer/tree/master?tab=readme-ov-file#comparison-to-berttokenizers) ë¹„êµ ê²°ê³¼, `FastBertTokenizer(C#)`ê°€ ë” ë¹ ë¥¸ ê²ƒìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤.
  * ğŸ”¥ [rust-tokenizers (Rust)](https://github.com/guillaume-be/rust-tokenizers)
    * BertTokenizerFlashì™€ Blingfireë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ, ë‹¤ë¥¸ êµ¬í˜„ì²´ë³´ë‹¤ëŠ” ë¹ ë¥´ê³  ì •í™•í•©ë‹ˆë‹¤.
    * ğŸ”µ **pipìœ¼ë¡œ ì„¤ì¹˜ ê°€ëŠ¥**
  * âŒ [tokenizers-cpp (C++)](https://github.com/mlc-ai/tokenizers-cpp)
    * SentencePiece ë° HuggingFaceì˜ Rust êµ¬í˜„ì²´ë¥¼ ë˜í•‘í•œ í˜•íƒœë¡œ, ë…ìì ì¸ ì„±ëŠ¥ ì¸¡ì •ì€ ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤.
  * âŒ [bertTokenizer (Java)](https://github.com/ankiteciitkgp/bertTokenizer)
    * Java êµ¬í˜„ì²´ëŠ” ì·¨ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  * âœ… [ZhuoruLin/fast-wordpiece (Rust)](https://github.com/ZhuoruLin/fast-wordpiece)
    * LinMaxMatching ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ Rust êµ¬í˜„ì²´ë¡œ, Rust í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, C++ êµ¬í˜„ì²´ë³´ë‹¤ ë¹ ë¥´ì§€ëŠ” ì•Šì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
  * âŒ [huggingface_tokenizer_cpp (C++)](https://github.com/Sorrow321/huggingface_tokenizer_cpp)
    * ë‹¨ìˆœí•œ C++ êµ¬í˜„ ë°©ì‹ìœ¼ë¡œ ì¸í•´ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.
  * âŒ [SeanLee97/BertWordPieceTokenizer.jl (Julia)](https://github.com/SeanLee97/BertWordPieceTokenizer.jl)
    * Julia êµ¬í˜„ì²´ëŠ” ì·¨ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

* **BPE**
  * https://github.com/openai/tiktoken

* **SentencePiece**
  * [google/sentencepiece (C++)](https://github.com/google/sentencepiece)



## â­ ê¸°ë¡

<a href="https://www.star-history.com/#NLPOptimize/flash-tokenizer&Date">

 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=NLPOptimize/flash-tokenizer&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=NLPOptimize/flash-tokenizer&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=NLPOptimize/flash-tokenizer&type=Date" />
 </picture>
</a>


## ì°¸ê³ 

* https://medium.com/@techhara/which-bert-tokenizer-is-faster-b832aa978b46
* https://medium.com/@atharv6f_47401/wordpiece-tokenization-a-bpe-variant-73cc48865cbf
* https://www.restack.io/p/transformer-models-bert-answer-fast-berttokenizerfast-cat-ai
* https://medium.com/@anmolkohli/my-notes-on-bert-tokenizer-and-model-98dc22d0b64
* https://nocomplexity.com/documents/fossml/nlpframeworks.html
* https://github.com/martinus/robin-hood-hashing
* https://arxiv.org/abs/2012.15524
* https://github.com/google/highway
